{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "279a4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#原始数据目录 \n",
    "h5s_path=r\"F:\\Workspace\\Projects\\LidarCloud\\CMA\\2025\"\n",
    "channel_keys = ['CH355', 'CH532', 'CH1064', 'PDR355', 'PDR532']\n",
    "#mask目录 格式为tif\n",
    "masks_path = h5s_path.replace('CMA', 'mask')\n",
    "#0 代表非云 1代表云 2代表不确定性较高的云\n",
    "#文件夹下结构如下\n",
    "#>yyyy\n",
    "#>>mmdd\n",
    "#>>>h5 files or masks(.tif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22e4de76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: 51628.tif -> 51628.h5 | +80000 rows\n",
      "OK: 53845.tif -> 53845.h5 | +92148 rows\n",
      "OK: 53845.tif -> 53845.h5 | +92148 rows\n",
      "OK: 58847.tif -> 58847.h5 | +81277 rows\n",
      "OK: 58847.tif -> 58847.h5 | +81277 rows\n",
      "OK: 51628.tif -> 51628.h5 | +80000 rows\n",
      "OK: 51628.tif -> 51628.h5 | +80000 rows\n",
      "OK: 53845.tif -> 53845.h5 | +80245 rows\n",
      "OK: 53845.tif -> 53845.h5 | +80245 rows\n",
      "Saved table: output\\feature_samples.parquet\n",
      "Saved stats: output\\feature_stats_by_class.csv\n",
      "Saved KS: output\\ks_scores.csv\n",
      "Saved histograms: output\\hists_top8_label01.png\n",
      "Done.\n",
      "Saved table: output\\feature_samples.parquet\n",
      "Saved stats: output\\feature_stats_by_class.csv\n",
      "Saved KS: output\\ks_scores.csv\n",
      "Saved histograms: output\\hists_top8_label01.png\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Lidar Cloud Pixel-Level Feature Exploration\n",
    "Author: Dr. Zou (pipeline scaffold prepared by GPT-5 Thinking)\n",
    "\n",
    "Goal\n",
    "----\n",
    "• Use existing mask .tif as the driver (because masks are fewer) and match H5 files.\n",
    "• Extract per-pixel features from provided channels + key derived features (gradients, filters, ratios).\n",
    "• Build a sampled feature table with labels {0: non-cloud, 1: cloud, 2: uncertain} for EDA/class-wise stats.\n",
    "• Save outputs:\n",
    "  - output/feature_samples.parquet (all sampled pixels with features)\n",
    "  - output/feature_stats_by_class.csv (summary statistics per class)\n",
    "  - logs/skip.log (mismatches or errors)\n",
    "\n",
    "Assumptions\n",
    "-----------\n",
    "• H5 datasets are 2D arrays (time × height) for each key in channel_keys.\n",
    "• Mask .tif aligns with the H5 grids. If shapes differ, the mask is resized by nearest-neighbor.\n",
    "• Directory layout:\n",
    "    h5s_path/YYYY/MMDD/*.h5\n",
    "    masks_path/YYYY/MMDD/*.tif\n",
    "• Station filtering by 51628, 53845, 58847 if station IDs appear in file names. If not present, files are still matched by date folder.\n",
    "• Default axes: time axis = 0, height axis = 1. Adjust TIME_AXIS/HEIGHT_AXIS if needed.\n",
    "\n",
    "Speed/Memory Notes\n",
    "------------------\n",
    "• Pure NumPy + SciPy ndimage (vectorized). No Python loops over pixels.\n",
    "• Optional random subsampling per class per (mask,H5) pair to keep tables manageable.\n",
    "• Multiprocessing ready for per-file parallelism if needed (Windows-safe spawn).\n",
    "\n",
    "Usage\n",
    "-----\n",
    "1) Adjust CONFIG below if necessary (paths, sampling, stations).\n",
    "2) Run:\n",
    "   python lidar_cloud_pixel_features.py\n",
    "3) Check `output/feature_stats_by_class.csv` and the Parquet for EDA.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from scipy.ndimage import gaussian_filter, sobel, uniform_filter, laplace\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======================\n",
    "# ====== CONFIG ========\n",
    "# ======================\n",
    "# --- Paths from user ---\n",
    "h5s_path = r\"F:\\Workspace\\Projects\\LidarCloud\\CMA\\2025\"\n",
    "channel_keys = ['CH355', 'CH532', 'CH1064', 'PDR355', 'PDR532']\n",
    "masks_path = h5s_path.replace('CMA', 'mask')  # expects same YYYY/MMDD tree\n",
    "\n",
    "# --- Stations of interest (optional filter in filenames) ---\n",
    "STATIONS = {\"51628\", \"53845\", \"58847\"}\n",
    "\n",
    "# --- Axes orientation (before transform) ---\n",
    "TIME_AXIS_RAW = 0\n",
    "HEIGHT_AXIS_RAW = 1\n",
    "# Apply transpose+vertical flip to align with mask (Z,T)\n",
    "APPLY_TRANSPOSE_FLIP = True\n",
    "TIME_AXIS = 1 if APPLY_TRANSPOSE_FLIP else 0\n",
    "HEIGHT_AXIS = 0 if APPLY_TRANSPOSE_FLIP else 1\n",
    "\n",
    "# --- Range correction & normalization for CH channels ---\n",
    "NORMALIZE_RANGE = {\n",
    "    \"51628\": {\"CH355\": 3.948e5, \"CH532\": 5.169e5, \"CH1064\": 2.283e5},\n",
    "    \"53845\": {\"CH355\": 4.158e5, \"CH532\": 4.750e5, \"CH1064\": 2.294e5},\n",
    "    \"58847\": {\"CH355\": 4.547e5, \"CH532\": 3.911e5, \"CH1064\": 2.321e5},\n",
    "}\n",
    "\n",
    "# --- Sampling control ---\n",
    "MAX_SAMPLES_PER_CLASS = 80000      # per (mask,h5) pair\n",
    "RANDOM_SEED = 1337\n",
    "\n",
    "# --- Output ---\n",
    "OUT_DIR = \"output\"\n",
    "LOG_DIR = \"logs\"\n",
    "TABLE_PATH = os.path.join(OUT_DIR, \"feature_samples.parquet\")\n",
    "STATS_PATH = os.path.join(OUT_DIR, \"feature_stats_by_class.csv\")\n",
    "KS_PATH = os.path.join(OUT_DIR, \"ks_scores.csv\")\n",
    "HISTS_PATH = os.path.join(OUT_DIR, \"hists_top8_label01.png\")\n",
    "SKIP_LOG = os.path.join(LOG_DIR, \"skip.log\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# =========================\n",
    "# ====== UTILITIES ========\n",
    "# =========================\n",
    "_station_re = re.compile(r\"(\\d{5})\")  # capture 5-digit station ID anywhere in name\n",
    "_ts_re = re.compile(r\"(\\d{8})(?:[_-]?(\\d{4,6}))?\")  # date YYYYMMDD optionally time\n",
    "\n",
    "\n",
    "def _read_mask(mask_path: str) -> np.ndarray:\n",
    "    \"\"\"Read mask tif as uint8/uint16 -> int array in {0,1,2}.\n",
    "    Returns shape (T, Z) to match H5 orientation assumption.\n",
    "    \"\"\"\n",
    "    img = Image.open(mask_path)\n",
    "    arr = np.array(img)\n",
    "    if arr.ndim == 3:\n",
    "        # use first channel if RGB accidentally\n",
    "        arr = arr[..., 0]\n",
    "    # ensure integer\n",
    "    arr = arr.astype(np.int16)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def _get_station_from_name(path: str) -> Optional[str]:\n",
    "    base = os.path.basename(path)\n",
    "    m = _station_re.search(base)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _get_date_from_parents(path: str) -> Optional[Tuple[str, str]]:\n",
    "    \"\"\"Infer (YYYY, MMDD) from the parent folders.\n",
    "    Expected: .../YYYY/MMDD/filename\n",
    "    \"\"\"\n",
    "    parts = os.path.normpath(path).split(os.sep)\n",
    "    if len(parts) < 3:\n",
    "        return None\n",
    "    yyyy, mmdd = parts[-3], parts[-2]\n",
    "    if len(yyyy) == 4 and len(mmdd) == 4 and yyyy.isdigit() and mmdd.isdigit():\n",
    "        return yyyy, mmdd\n",
    "    return None\n",
    "\n",
    "\n",
    "def _list_mask_files(root: str) -> List[str]:\n",
    "    return sorted(glob.glob(os.path.join(root, \"*\", \"*\", \"*.tif\")))\n",
    "\n",
    "\n",
    "def _list_h5_candidates(h5_root: str, yyyy: str, mmdd: str) -> List[str]:\n",
    "    return sorted(glob.glob(os.path.join(h5_root, yyyy, mmdd, \"*.h5\")))\n",
    "\n",
    "\n",
    "def _choose_h5_for_mask(mask_path: str, h5_candidates: List[str]) -> Optional[str]:\n",
    "    \"\"\"Pick H5 file matching by station ID if possible; otherwise first candidate.\n",
    "    Heuristic: match station; if multiple, try closest timestamp parsed from name.\n",
    "    \"\"\"\n",
    "    if not h5_candidates:\n",
    "        return None\n",
    "    mask_station = _get_station_from_name(mask_path)\n",
    "\n",
    "    def _parse_ts(p: str) -> Optional[datetime]:\n",
    "        base = os.path.basename(p)\n",
    "        m = _ts_re.search(base)\n",
    "        if not m:\n",
    "            return None\n",
    "        ymd, hm = m.group(1), m.group(2)\n",
    "        try:\n",
    "            if hm:\n",
    "                # allow HHMM or HHMMSS\n",
    "                if len(hm) == 4:\n",
    "                    return datetime.strptime(ymd + hm, \"%Y%m%d%H%M\")\n",
    "                elif len(hm) == 6:\n",
    "                    return datetime.strptime(ymd + hm, \"%Y%m%d%H%M%S\")\n",
    "            return datetime.strptime(ymd, \"%Y%m%d\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # filter by station first\n",
    "    cand = h5_candidates\n",
    "    if mask_station and any(mask_station in os.path.basename(x) for x in cand):\n",
    "        cand = [x for x in cand if mask_station in os.path.basename(x)]\n",
    "        if not cand:\n",
    "            cand = h5_candidates\n",
    "\n",
    "    # if multiple, choose by closest timestamp to mask name's timestamp (if any)\n",
    "    mask_ts = _parse_ts(mask_path)\n",
    "    if mask_ts:\n",
    "        cand_with_dt = [(p, _parse_ts(p)) for p in cand]\n",
    "        cand_with_dt = [(p, dt) for (p, dt) in cand_with_dt if dt is not None]\n",
    "        if cand_with_dt:\n",
    "            p_best = min(cand_with_dt, key=lambda kv: abs((kv[1] - mask_ts).total_seconds()))[0]\n",
    "            return p_best\n",
    "\n",
    "    return cand[0]\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ====== FEATURES ==========\n",
    "# ==========================\n",
    "\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    keys: List[str]\n",
    "    time_axis: int = TIME_AXIS\n",
    "    height_axis: int = HEIGHT_AXIS\n",
    "    # window for local stats\n",
    "    local_win: int = 5\n",
    "    # gaussian sigmas\n",
    "    sigma1: float = 1.0\n",
    "    sigma2: float = 2.0\n",
    "\n",
    "\n",
    "def safe_log1p(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    x = np.clip(x, a_min=0.0, a_max=None)  # lidar backscatter is non-negative in general\n",
    "    return np.log1p(x)\n",
    "\n",
    "\n",
    "def robust_scale(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Median / MAD scaling (robust z-score).\"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    med = np.nanmedian(x)\n",
    "    mad = np.nanmedian(np.abs(x - med))\n",
    "    if mad <= 1e-8:\n",
    "        return (x - med)\n",
    "    return (x - med) / (1.4826 * mad)\n",
    "\n",
    "\n",
    "def local_mean_var(x: np.ndarray, size: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    mu = uniform_filter(x, size=size)\n",
    "    mu2 = uniform_filter(x * x, size=size)\n",
    "    var = np.maximum(mu2 - mu * mu, 0.0)\n",
    "    return mu, var\n",
    "\n",
    "\n",
    "def derive_single_channel_features(name: str, arr: np.ndarray, cfg: FeatureConfig) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Compute key 2D features for one channel.\n",
    "    Returns a dict of feature_name -> 2D array (same shape as arr).\n",
    "    \"\"\"\n",
    "    feat = {}\n",
    "    X = safe_log1p(arr)\n",
    "    Xr = robust_scale(X)\n",
    "\n",
    "    # Gaussian blur and Difference of Gaussians\n",
    "    g1 = gaussian_filter(Xr, sigma=cfg.sigma1)\n",
    "    g2 = gaussian_filter(Xr, sigma=cfg.sigma2)\n",
    "    dog = g1 - g2\n",
    "\n",
    "    # Sobel along time & height (axis order matters)\n",
    "    # sobel operates per-axis when axis is specified\n",
    "    sob_t = sobel(Xr, axis=cfg.time_axis)\n",
    "    sob_z = sobel(Xr, axis=cfg.height_axis)\n",
    "    grad_mag = np.hypot(sob_t, sob_z)\n",
    "\n",
    "    # Laplacian (second-order)\n",
    "    lap = laplace(Xr)\n",
    "\n",
    "    # Local statistics\n",
    "    mu, var = local_mean_var(Xr, size=cfg.local_win)\n",
    "    std = np.sqrt(var + 1e-8)\n",
    "    local_contrast = (Xr - mu) / (std + 1e-6)\n",
    "\n",
    "    # Pack\n",
    "    feat[f\"{name}_log1p\"] = X\n",
    "    feat[f\"{name}_robust\"] = Xr\n",
    "    feat[f\"{name}_gauss_s{cfg.sigma1}\"] = g1\n",
    "    feat[f\"{name}_dog_{cfg.sigma1}_{cfg.sigma2}\"] = dog\n",
    "    feat[f\"{name}_sob_t\"] = sob_t\n",
    "    feat[f\"{name}_sob_z\"] = sob_z\n",
    "    feat[f\"{name}_gradmag\"] = grad_mag\n",
    "    feat[f\"{name}_laplace\"] = lap\n",
    "    feat[f\"{name}_lmean_w{cfg.local_win}\"] = mu\n",
    "    feat[f\"{name}_lstd_w{cfg.local_win}\"] = std\n",
    "    feat[f\"{name}_lcontrast_w{cfg.local_win}\"] = local_contrast\n",
    "    return feat\n",
    "\n",
    "\n",
    "def derive_cross_channel_features(data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Cross-channel color ratios and depol combinations (safe divisions).\"\"\"\n",
    "    out = {}\n",
    "\n",
    "    def sdiv(a, b):\n",
    "        return np.divide(a, b, out=np.zeros_like(a, dtype=np.float32), where=(b != 0))\n",
    "\n",
    "    ch355 = data.get('CH355')\n",
    "    ch532 = data.get('CH532')\n",
    "    ch1064 = data.get('CH1064')\n",
    "    pdr355 = data.get('PDR355')\n",
    "    pdr532 = data.get('PDR532')\n",
    "\n",
    "    if ch355 is not None and ch532 is not None:\n",
    "        out['ratio_532_355'] = sdiv(ch532, ch355)\n",
    "        out['logratio_532_355'] = safe_log1p(out['ratio_532_355'])\n",
    "    if ch532 is not None and ch1064 is not None:\n",
    "        out['ratio_1064_532'] = sdiv(ch1064, ch532)\n",
    "        out['logratio_1064_532'] = safe_log1p(out['ratio_1064_532'])\n",
    "    if ch355 is not None and ch1064 is not None:\n",
    "        out['ratio_1064_355'] = sdiv(ch1064, ch355)\n",
    "        out['logratio_1064_355'] = safe_log1p(out['ratio_1064_355'])\n",
    "\n",
    "    # Depol × intensity (can emphasize ice/edges)\n",
    "    if pdr532 is not None and ch532 is not None:\n",
    "        out['pdr532_x_ch532'] = pdr532 * safe_log1p(ch532)\n",
    "    if pdr355 is not None and ch355 is not None:\n",
    "        out['pdr355_x_ch355'] = pdr355 * safe_log1p(ch355)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# ====== PIPELINE ==========\n",
    "# ==========================\n",
    "\n",
    "# ---------- Height helpers ----------\n",
    "_DEF_HEIGHT_KEYS = [\n",
    "    'height', 'range', 'altitude', 'HEIGHT', 'RANGE', 'ALTITUDE', 'z', 'Z',\n",
    "]\n",
    "\n",
    "def _read_height_vector(f: h5py.File, height_len: int) -> np.ndarray:\n",
    "    for k in _DEF_HEIGHT_KEYS:\n",
    "        if k in f:\n",
    "            h = np.array(f[k][...]).squeeze()\n",
    "            if h.ndim == 1 and h.size == height_len:\n",
    "                h = h.astype(np.float32)\n",
    "                h = np.maximum(h, 0.0)\n",
    "                return h\n",
    "    return np.arange(height_len, dtype=np.float32)\n",
    "\n",
    "\n",
    "def _read_h5_channels_with_corrections(h5_path: str, keys: List[str], station: Optional[str]) -> Tuple[Dict[str, np.ndarray], np.ndarray]:\n",
    "    \"\"\"Read channels, apply CH range^2 correction + station normalization, then transpose+flip to (Z,T).\"\"\"\n",
    "    out: Dict[str, np.ndarray] = {}\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        avail = [k for k in keys if k in f]\n",
    "        if not avail:\n",
    "            return {}, np.array([])\n",
    "        probe = f[avail[0]][...]\n",
    "        if probe.ndim != 2:\n",
    "            raise ValueError(f\"Dataset {avail[0]} in {h5_path} is not 2D.\")\n",
    "        T = probe.shape[TIME_AXIS_RAW]\n",
    "        Z = probe.shape[HEIGHT_AXIS_RAW]\n",
    "        height = _read_height_vector(f, Z).astype(np.float32)\n",
    "        height_sq = np.square(height)\n",
    "        for k in avail:\n",
    "            arr = np.array(f[k][...], dtype=np.float32)\n",
    "            if arr.ndim != 2:\n",
    "                raise ValueError(f\"Dataset {k} in {h5_path} is not 2D.\")\n",
    "            if k.startswith('CH'):\n",
    "                if HEIGHT_AXIS_RAW == 1:\n",
    "                    arr = arr * height_sq[None, :]\n",
    "                else:\n",
    "                    arr = arr * height_sq[:, None]\n",
    "                max_map = NORMALIZE_RANGE.get(station or \"\", {})\n",
    "                max_val = max_map.get(k)\n",
    "                if max_val and max_val > 0:\n",
    "                    arr = arr / float(max_val)\n",
    "            if arr.ndim == 2:\n",
    "                arr = np.transpose(arr, (1, 0))[::-1, :]\n",
    "            out[k] = arr\n",
    "        h_after = height[::-1] if APPLY_TRANSPOSE_FLIP else height\n",
    "        return out, h_after\n",
    "\n",
    "\n",
    "def _compute_stats_and_ks(table_path: str):\n",
    "    \"\"\"Compute per-class stats; rank features by separability using KS, symmetric KL, and JSD.\n",
    "    Also plot overlaid *normalized* histograms (probability densities) for the top-8 by JSD.\n",
    "    \"\"\"\n",
    "    full = pd.read_parquet(table_path)\n",
    "    full = full[full['label'].isin([0, 1])]\n",
    "\n",
    "    feature_cols = [c for c in full.columns if c not in {'label', 'station', 'yyyy', 'mmdd', 'mask_file', 'h5_file'}]\n",
    "    feature_cols = [c for c in feature_cols if np.issubdtype(full[c].dtype, np.number)]\n",
    "\n",
    "    # ---- Class-wise summary stats (unchanged) ----\n",
    "    def summarize(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        desc = group[feature_cols].describe(percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]).T\n",
    "        desc = desc.rename(columns={'50%': 'p50', '25%': 'p25', '75%': 'p75', '5%': 'p05', '95%': 'p95'})\n",
    "        keep = ['count', 'mean', 'std', 'min', 'p05', 'p25', 'p50', 'p75', 'p95', 'max']\n",
    "        return desc[keep]\n",
    "\n",
    "    stats = full.groupby('label', as_index=True).apply(summarize)\n",
    "    stats.index = [f\"label{lbl}:{name}\" for lbl, name in stats.index]\n",
    "    stats.to_csv(STATS_PATH)\n",
    "\n",
    "    x0 = full[full['label'] == 0]\n",
    "    x1 = full[full['label'] == 1]\n",
    "\n",
    "    # ---- Divergence helpers (histogram-based) ----\n",
    "    def _pmf(a: np.ndarray, bins: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "        # probability mass on common bins\n",
    "        cnt, _ = np.histogram(a, bins=bins, density=False)\n",
    "        p = cnt.astype(np.float64) + eps\n",
    "        s = p.sum()\n",
    "        return p / s if s > 0 else np.full_like(p, 1.0 / len(p), dtype=np.float64)\n",
    "\n",
    "    def _kl(p: np.ndarray, q: np.ndarray) -> float:\n",
    "        return float(np.sum(p * np.log(p / q)))\n",
    "\n",
    "    def _jsd(p: np.ndarray, q: np.ndarray) -> float:\n",
    "        m = 0.5 * (p + q)\n",
    "        return 0.5 * _kl(p, m) + 0.5 * _kl(q, m)\n",
    "\n",
    "    rows = []\n",
    "    for col in feature_cols:\n",
    "        a = x0[col].values\n",
    "        b = x1[col].values\n",
    "        a = a[np.isfinite(a)]\n",
    "        b = b[np.isfinite(b)]\n",
    "        if len(a) < 50 or len(b) < 50:\n",
    "            continue\n",
    "        # KS on raw samples\n",
    "        ks = ks_2samp(a, b, alternative='two-sided', mode='auto')\n",
    "\n",
    "        # Common bins based on robust percentiles\n",
    "        try:\n",
    "            pmin = np.nanmin([np.percentile(a, 1), np.percentile(b, 1)])\n",
    "            pmax = np.nanmax([np.percentile(a, 99), np.percentile(b, 99)])\n",
    "        except Exception:\n",
    "            pmin, pmax = np.nanmin(np.concatenate([a, b])), np.nanmax(np.concatenate([a, b]))\n",
    "        if not np.isfinite(pmin) or not np.isfinite(pmax) or pmin >= pmax:\n",
    "            # degenerate feature; skip\n",
    "            continue\n",
    "        bins = np.linspace(pmin, pmax, 64)\n",
    "\n",
    "        # PMFs + divergences\n",
    "        p = _pmf(a, bins)\n",
    "        q = _pmf(b, bins)\n",
    "        kl_pq = _kl(p, q)\n",
    "        kl_qp = _kl(q, p)\n",
    "        sym_kl = 0.5 * (kl_pq + kl_qp)\n",
    "        jsd = _jsd(p, q)\n",
    "        # entropies (nats)\n",
    "        H0 = float(-np.sum(p * np.log(p)))\n",
    "        H1 = float(-np.sum(q * np.log(q)))\n",
    "\n",
    "        rows.append((col, float(ks.statistic), float(ks.pvalue), sym_kl, kl_pq, kl_qp, jsd, H0, H1))\n",
    "\n",
    "    score_cols = ['feature', 'ks_stat', 'ks_pvalue', 'sym_kl', 'kl_pq', 'kl_qp', 'jsd', 'H0', 'H1']\n",
    "    score_df = pd.DataFrame(rows, columns=score_cols)\n",
    "    if not score_df.empty:\n",
    "        # Prefer features with larger JSD (more separable). Use KS as secondary key.\n",
    "        score_df = score_df.sort_values(['jsd', 'ks_stat'], ascending=[False, False])\n",
    "    score_df.to_csv(KS_PATH, index=False)\n",
    "\n",
    "    # ---- Plot: overlaid *normalized* histograms for top-8 by JSD ----\n",
    "    top = score_df.head(8)\n",
    "    n = len(top)\n",
    "    if n > 0:\n",
    "        cols = 2\n",
    "        rows_n = math.ceil(n / cols)\n",
    "        fig, axes = plt.subplots(rows_n, cols, figsize=(10, 3.2 * rows_n), constrained_layout=True)\n",
    "        if rows_n == 1:\n",
    "            axes = np.array(axes).reshape(1, -1)\n",
    "        for i, row in enumerate(top.itertuples(index=False)):\n",
    "            fname, ks_val, pv, symkl, klpq, klqp, jsd_val, H0, H1 = row\n",
    "            r, c = divmod(i, cols)\n",
    "            ax = axes[r, c]\n",
    "            a = x0[fname].values\n",
    "            b = x1[fname].values\n",
    "            a = a[np.isfinite(a)]\n",
    "            b = b[np.isfinite(b)]\n",
    "            # Common bins for visualization (same as above rule)\n",
    "            try:\n",
    "                pmin = np.nanmin([np.percentile(a, 1), np.percentile(b, 1)])\n",
    "                pmax = np.nanmax([np.percentile(a, 99), np.percentile(b, 99)])\n",
    "            except Exception:\n",
    "                pmin, pmax = np.nanmin(np.concatenate([a, b])), np.nanmax(np.concatenate([a, b]))\n",
    "            if not np.isfinite(pmin) or not np.isfinite(pmax) or pmin >= pmax:\n",
    "                pmin, pmax = np.nanmin(np.concatenate([a, b])), np.nanmax(np.concatenate([a, b]))\n",
    "            bins = np.linspace(pmin, pmax, 40)\n",
    "            # density=True -> probability density (area≈1)\n",
    "            ax.hist(a, bins=bins, density=True, alpha=0.5, label='label=0')\n",
    "            ax.hist(b, bins=bins, density=True, alpha=0.5, label='label=1')\n",
    "            ax.set_title(f\"{fname}KS={ks_val:.3f} | SymKL={symkl:.3f} | JSD={jsd_val:.3f}\")\n",
    "            ax.grid(True, ls=':', alpha=0.4)\n",
    "            if i % cols == 0:\n",
    "                ax.set_ylabel('prob. density')\n",
    "            ax.legend()\n",
    "        # hide unused subplots\n",
    "        for j in range(n, rows_n * cols):\n",
    "            r, c = divmod(j, cols)\n",
    "            fig.delaxes(axes[r, c])\n",
    "        fig.suptitle('Top-8 Features by JSD (label 0 vs 1)', y=1.02)\n",
    "        fig.savefig(HISTS_PATH, dpi=180)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # ---- 3D scatter: top-3 features by average entropy (Havg) ----\n",
    "        if not score_df.empty:\n",
    "            score_df['Havg'] = 0.5 * (score_df['H0'] + score_df['H1'])\n",
    "            top3 = score_df.sort_values('Havg', ascending=False).head(3)\n",
    "            if len(top3) == 3:\n",
    "                f1, f2, f3 = top3['feature'].tolist()\n",
    "                cols_needed = ['label', f1, f2, f3]\n",
    "                # Build balanced, downsampled subset for plotting\n",
    "                sub = full[cols_needed].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "                # Clip extremes to 1-99% per feature for visibility\n",
    "                for f in (f1, f2, f3):\n",
    "                    lo, hi = np.percentile(sub[f], 1), np.percentile(sub[f], 99)\n",
    "                    if np.isfinite(lo) and np.isfinite(hi) and lo < hi:\n",
    "                        sub[f] = np.clip(sub[f], lo, hi)\n",
    "                # Balanced sampling\n",
    "                max_points = 40000\n",
    "                per_cls = max_points // 2\n",
    "                sub0 = sub[sub['label'] == 0]\n",
    "                sub1 = sub[sub['label'] == 1]\n",
    "                if len(sub0) > per_cls:\n",
    "                    sub0 = sub0.sample(per_cls, random_state=1337)\n",
    "                if len(sub1) > per_cls:\n",
    "                    sub1 = sub1.sample(per_cls, random_state=1337)\n",
    "                from mpl_toolkits.mplot3d import Axes3D # noqa: F401\n",
    "                fig = plt.figure(figsize=(9, 7))\n",
    "                ax = fig.add_subplot(111, projection='3d')\n",
    "                ax.scatter(sub0[f1], sub0[f2], sub0[f3], s=2, alpha=0.35, label='label=0')\n",
    "                ax.scatter(sub1[f1], sub1[f2], sub1[f3], s=2, alpha=0.35, label='label=1')\n",
    "                ax.set_xlabel(f1)\n",
    "                ax.set_ylabel(f2)\n",
    "                ax.set_zlabel(f3)\n",
    "                ax.set_title(f'3D Scatter by Top-3 Entropy Features{f1}, {f2}, {f3}')\n",
    "                ax.legend(loc='best')\n",
    "                scatter_path = os.path.join(OUT_DIR, 'scatter3d_top3_entropy.png')\n",
    "                fig.savefig(scatter_path, dpi=180, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "def _resize_mask_if_needed(mask: np.ndarray, target_shape: Tuple[int, int]) -> np.ndarray:\n",
    "    if mask.shape == target_shape:\n",
    "        return mask\n",
    "    # nearest-neighbor resize via PIL; ensure (width,height) order\n",
    "    # current arrays are (T, Z); we map to (width,height)=(Z,T)\n",
    "    img = Image.fromarray(mask.astype(np.int16))\n",
    "    img = img.resize((target_shape[1], target_shape[0]), resample=Image.NEAREST)\n",
    "    arr = np.array(img).astype(np.int16)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def _read_h5_channels(h5_path: str, keys: List[str]) -> Dict[str, np.ndarray]:\n",
    "    out: Dict[str, np.ndarray] = {}\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        for k in keys:\n",
    "            if k in f:\n",
    "                arr = f[k][...]\n",
    "                if arr.ndim != 2:\n",
    "                    raise ValueError(f\"Dataset {k} in {h5_path} is not 2D.\")\n",
    "                out[k] = arr.astype(np.float32)\n",
    "            else:\n",
    "                # silently skip missing keys\n",
    "                pass\n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_features_for_pair(mask_path: str, h5_path: str, cfg: FeatureConfig,\n",
    "                              max_samples_per_class: int = MAX_SAMPLES_PER_CLASS) -> pd.DataFrame:\n",
    "    # read mask and channels\n",
    "    mask = _read_mask(mask_path)\n",
    "    station = _get_station_from_name(mask_path) or _get_station_from_name(h5_path) or \"unknown\"\n",
    "    ch_data, h_vec = _read_h5_channels_with_corrections(h5_path, cfg.keys, station)\n",
    "    if not ch_data:\n",
    "        raise RuntimeError(f\"No requested channels found in {h5_path}\")\n",
    "\n",
    "    # Ensure shapes consistent\n",
    "    any_arr = next(iter(ch_data.values()))\n",
    "    mask = _resize_mask_if_needed(mask, target_shape=any_arr.shape)\n",
    "\n",
    "    # Cross-channel features use corrected arrays\n",
    "    cross = derive_cross_channel_features(ch_data)\n",
    "\n",
    "    # Derive per-channel features\n",
    "    derived: Dict[str, np.ndarray] = {}\n",
    "    for k, arr in ch_data.items():\n",
    "        derived.update(derive_single_channel_features(k, arr, cfg))\n",
    "\n",
    "    # Merge everything (including original channels as log/robust already present)\n",
    "    all_feat_maps: Dict[str, np.ndarray] = {}\n",
    "    all_feat_maps.update(derived)\n",
    "    all_feat_maps.update(cross)\n",
    "    #添加原始值\n",
    "\n",
    "    for k, arr in ch_data.items():\n",
    "        all_feat_maps[f\"{k}_orig\"] = arr\n",
    "\n",
    "    # Flatten\n",
    "    labels = mask.reshape(-1)\n",
    "    valid = (labels == 0) | (labels == 1)\n",
    "\n",
    "    # Build a dict of flattened features\n",
    "    flat_features = {}\n",
    "    for name, arr in all_feat_maps.items():\n",
    "        if arr.shape != mask.shape:\n",
    "            # safety: align by resizing feature map (rare)\n",
    "            arr = _resize_mask_if_needed(arr, target_shape=mask.shape)\n",
    "        flat_features[name] = arr.reshape(-1)\n",
    "\n",
    "    # Stack into DataFrame (only valid labels)\n",
    "    df = pd.DataFrame({**flat_features, 'label': labels})\n",
    "    df = df[valid]\n",
    "\n",
    "    # Subsample per class to balance & limit size\n",
    "    out_parts = []\n",
    "    for cls in (0, 1):\n",
    "        part = df[df['label'] == cls]\n",
    "        if len(part) == 0:\n",
    "            continue\n",
    "        if len(part) > max_samples_per_class:\n",
    "            idx = np.random.choice(part.index.values, size=max_samples_per_class, replace=False)\n",
    "            part = part.loc[idx]\n",
    "        out_parts.append(part)\n",
    "    if not out_parts:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out_df = pd.concat(out_parts, axis=0, ignore_index=True)\n",
    "\n",
    "    # Add meta\n",
    "    yyyy_mmdd = _get_date_from_parents(mask_path)\n",
    "    station = _get_station_from_name(mask_path) or _get_station_from_name(h5_path) or \"unknown\"\n",
    "    out_df['station'] = station\n",
    "    if yyyy_mmdd:\n",
    "        out_df['yyyy'] = yyyy_mmdd[0]\n",
    "        out_df['mmdd'] = yyyy_mmdd[1]\n",
    "    out_df['mask_file'] = os.path.basename(mask_path)\n",
    "    out_df['h5_file'] = os.path.basename(h5_path)\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def _append_parquet(df: pd.DataFrame, path: str):\n",
    "    # Append by reading old then concat (simple & portable); for large-scale, consider pyarrow parquet appends.\n",
    "    if os.path.exists(path):\n",
    "        old = pd.read_parquet(path)\n",
    "        df = pd.concat([old, df], ignore_index=True)\n",
    "    df.to_parquet(path, index=False)\n",
    "\n",
    "\n",
    "def run_pipeline():\n",
    "    cfg = FeatureConfig(keys=channel_keys)\n",
    "    mask_files = _list_mask_files(masks_path)\n",
    "\n",
    "    if not mask_files:\n",
    "        print(f\"No mask files found under: {masks_path}\")\n",
    "        return\n",
    "\n",
    "    # Process each mask file\n",
    "    n_pairs = 0\n",
    "    with open(SKIP_LOG, 'w', encoding='utf-8') as flog:\n",
    "        for mask_path in mask_files[:5]:\n",
    "            # station filter if station explicitly in filename\n",
    "            station = _get_station_from_name(mask_path)\n",
    "            if station and STATIONS and (station not in STATIONS):\n",
    "                continue\n",
    "\n",
    "            ymd = _get_date_from_parents(mask_path)\n",
    "            if not ymd:\n",
    "                flog.write(f\"[SKIP] Cannot infer YYYY/MMDD from: {mask_path}\\n\")\n",
    "                continue\n",
    "            yyyy, mmdd = ymd\n",
    "            candidates = _list_h5_candidates(h5s_path, yyyy, mmdd)\n",
    "            if not candidates:\n",
    "                flog.write(f\"[SKIP] No H5 candidates for date {yyyy}/{mmdd} for mask: {mask_path}\\n\")\n",
    "                continue\n",
    "\n",
    "            h5_path = _choose_h5_for_mask(mask_path, candidates)\n",
    "            if h5_path is None:\n",
    "                flog.write(f\"[SKIP] Cannot select H5 for mask: {mask_path}\\n\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df = extract_features_for_pair(mask_path, h5_path, cfg, max_samples_per_class=MAX_SAMPLES_PER_CLASS)\n",
    "                if df.empty:\n",
    "                    flog.write(f\"[SKIP] Empty features for pair: {mask_path} | {h5_path}\\n\")\n",
    "                    continue\n",
    "                # Persist incrementally to avoid huge memory\n",
    "                _append_parquet(df, TABLE_PATH)\n",
    "                n_pairs += 1\n",
    "                print(f\"OK: {os.path.basename(mask_path)} -> {os.path.basename(h5_path)} | +{len(df)} rows\")\n",
    "            except Exception as e:\n",
    "                flog.write(f\"[ERR] {mask_path} | {h5_path} | {repr(e)}\\n\")\n",
    "\n",
    "    if n_pairs == 0:\n",
    "        print(\"No valid (mask, H5) pairs processed. See logs/skip.log for details.\")\n",
    "        return\n",
    "\n",
    "    # KS & stats on the aggregated table\n",
    "    _compute_stats_and_ks(TABLE_PATH)\n",
    "\n",
    "    print(f\"Saved table: {TABLE_PATH}\")\n",
    "    print(f\"Saved stats: {STATS_PATH}\")\n",
    "    print(f\"Saved KS: {KS_PATH}\")\n",
    "    print(f\"Saved histograms: {HISTS_PATH}\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ddb954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
